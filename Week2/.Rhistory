install.packages("plyr")
install.packages("ggplot2")
g = rnorm(10,5)
plot(g)
pwd()
getwd()
install.packages("UsingR")
ls -l
exit()
install.packages(c("boot", "class", "cluster", "codetools", "colorspace", "evaluate", "foreign", "formatR", "Formula", "ggplot2", "highr", "Hmisc", "jsonlite", "KernSmooth", "knitr", "lattice", "manipulate", "markdown", "MASS", "Matrix", "mgcv", "mime", "nlme", "nnet", "plyr", "Rcpp", "RCurl", "rpart", "spatial", "stringr", "survival"))
library(data.table)
library(logging)
library(RSQLite)
install.packages("data.table", "logging", "RSQLite")
install.packages("data.table")
install.packages('logging')
install.packages(RSQLite)
install.packages("RSQLite")
library(data.table)
library(logging)
library(RSQLite)
list.dirs
A_matrix = matrix(4, nrow=4, ncol=3) # Makes use of broadcasting
View(A_matrix)
B_matrix = matrix(1:50, nrow=4, ncol=3)
View(B_matrix)
A_matrix + B_matrix
A_matrix * B_matrix
A_matrix %*% B_matrix
A_matrix %*% t(B_matrix)
x_values = seq(from=as.Date('2015-01-01'),
to=as.Date('2015-02-12'),
by = 3)
df = data.frame('dates' = x_values,
'x1'    = runif(15,-10,20),
'x2'    = 1:15,
'x3'    = strsplit('MississippiMath','')[[1]])
View(df)
df$x3 = as.character(df$x3)
df$x3 = tolower(df$x3)
View(df)
str(df)
head(df)
tail(df, n=10)
df = as.data.table(df)
View(df)
df[,sum(x1)]
df[,c(sum(x1),sd(x2))]
?logging
x_values
install.packages("XML")
nfl_site = "http://www.usatoday.com/sports/nfl/arrests/"
nfl_html = readHTMLTable(nfl_site)
nfl_site = "http://www.usatoday.com/sports/nfl/arrests/"
library(XML)
nfl_html = readHTMLTable(nfl_site)
nfl_html = readHTMLTable(nfl_site)
nfl_html
nfl_data = nfl_html[[1]]
nfl_html
db_file = 'test_db.db'
conn = dbConnect(dbDriver("SQLite"), dbname=db_file)
dbWriteTable(conn, 'table_name', medals_data, overwrite=TRUE)
medals_data <- read.table("medals.txt", sep="\t", header=TRUE)
p = 0.75
n = 1000
?rbinom
bern_samples = rbinom(n, 1, p)
bern_sample_mean = sum(bern_samples)/length(bern_samples)
bern_sample_var = bern_sample_mean * (1-bern_sample_mean)
##--------------------------------------------
##
## Exploring Data in R (lecture 1)
##
## Class: PCE Data Science Methods Class
##
## Contains examples of:
##
## -Working with Distributions
##
## -Visually/Numerically Exploring data
##
##--------------------------------------------
##-----Exploring data Visually----
# Bernoulli (Binomial with n = 1)
p = 0.75
n = 1000
bern_samples = rbinom(n, 1, p)
bern_sample_mean = sum(bern_samples)/length(bern_samples)
bern_sample_var = bern_sample_mean * (1-bern_sample_mean)
bern_var = p*(1-p)
# Binomial
N = c(5, 25, 75)
binom_samples = lapply(N, function(x) rbinom(n, x, p))
binom_sample_means = lapply(binom_samples, mean)
binom_means = N*p
binom_sample_vars = lapply(binom_samples, var)
binom_vars = N*p*(1-p)
par(mfrow=c(2,2))
for (i in 1:3){
hist(binom_samples[[i]], main=paste(N[i],'Experiments'), freq=FALSE)
x_norm = seq(0,N[i], by = 0.025)
y_norm = dnorm(x_norm, mean=binom_means[i], sd=sqrt(binom_vars[i]))
lines(x_norm, y_norm)
}
install.packages("data.table")
install.packages("dplyr")
library(data.table)
library(data.table)
install.packages("MASS")
##--------------------------------------------
##
## R Coding Review
##
## Class: PCE Data Science Methods Class
##
## Creator: Nick McClure (nickmc@uw.edu)
##
##--------------------------------------------
getwd()
setwd('E:/Work/Teaching/PCE_Data_Science/1_Intro_Lecture')
##-----Functions------
headcount = read.csv('JitteredHeadCount.csv', stringsAsFactors=FALSE)  # Read in headcount file
str(headcount) # Look at structure of data
# Convert dates:
headcount$DateFormat = as.Date(headcount$DateFormat, format = "%m/%d/%Y")
# See
?strptime # for representations of dates/datetimes
# The purpose of functions are to:
#    1.  Make code readable.
#    2.  Make code reusable.  Do NOT copy and paste code! (evil stare from Linus Torvalds)
# Let's make a season computation:
#
# 1 = Winter: months 1-3
# 2 = Spring: months 4-6
# 3 = Summer: months 7-9
# 4 = Fall:   months 10-12
seasons = c('winter', 'spring', 'summer', 'fall') # we will use this later
# Ugly way:
season_vector = c() # Initialization
for (row in 1:nrow(headcount)){ # Loop through rows
month_temp_string = format(headcount$DateFormat[row], format="%m")
month_temp = as.numeric(month_temp_string)
if ( month_temp %in% (1:3) ){                # if month is in 1,2,3 then winter
season_vector = c(season_vector,'winter')  #    then add 'winter' to vector
}else if ( month_temp %in% (4:6) ){          # if month is in 4,5,6 then spring
season_vector = c(season_vector,'spring')  #    then add 'spring' to vector...
}else if ( month_temp %in% (4:6) ){
season_vector = c(season_vector,'summer')
}else{
season_vector = c(season_vector,'fall')
}
}# End row loop (row)
headcount$season_ugly = season_vector # Assign outcome to dataframe
# Put this in a function:
get_season_from_date = function(date_value){
month_temp = as.numeric(format(date_value, format="%m"))
if ( month_temp %in% (1:3) ){
season = 'winter'
}else if ( month_temp %in% (4:6) ){
season = 'spring'
}else if ( month_temp %in% (7:9) ){
season = 'summer'
}else{
season = 'fall'
}
return(season)
}
# Our for loop now becomes:
season_vector = c() # Initialization
for (row in 1:nrow(headcount)){ # Loop through rows
season = get_season_from_date(headcount$DateFormat[row])
season_vector = c(season_vector, season)
}
# clean up
rm(season_vector)   # Remove stuff we don't need
gc()                # Force R to clean up the memory
invisible(gc())     # Don't really care about memory stats at this level
invisible(gc());invisible(gc());invisible(gc());invisible(gc());invisible(gc());
# Better, but still so slow!  Let's get rid of the for loop entirely
headcount$season_fast = sapply(headcount$DateFormat, function(x) get_season_from_date(x))
# or better yet:
headcount$season_fast = sapply(headcount$DateFormat, get_season_from_date)
# faster way:
headcount$month = as.numeric(format(headcount$DateFormat, format="%m")) # Set a month integer
headcount$season_faster = ''  # Initialize
headcount$season_faster[headcount$month %in% (1:3)] = 'Winter'
headcount$season_faster[headcount$month %in% (4:6)] = 'Spring'
headcount$season_faster[headcount$month %in% (7:9)] = 'Summer'
headcount$season_faster[headcount$month %in% (10:12)] = 'Fall'
# Let's see if we can speed it up
library(data.table)
headcount = as.data.table(headcount) # Make headcount a data.table
# Here is a trick:
#   Note the following:
(1:12)  # A sequence of months, 1 to 12
ceiling((1:12)/3) # Converted to an integer, 1 = Winter, ... 4 = Fall.
seasons[ceiling((1:12)/3)] # Use these integers as indices in our seasons vector
# Now stick the above trick in data.table format:
# headcount = headcount[ ,newcolumn := <formula for new column here>, ]
headcount = headcount[ ,season_fastest := seasons[ceiling(as.numeric(format(DateFormat, format="%m"))/3)], ]
##-----Loading/Reading data-----
?read.table  # read.table is the way R reads all files.
?readLines  # One way to access html connections
stack_html = readLines('http://www.stackoverflow.com')
# Find all the lines that contain the phrase " vote"
lines_with_vote = grep(" vote" ,stack_html)
# Look at top few lines with 'vote'
stack_html[head(lines_with_vote)]
# Look at end lines with 'vote'
stack_html[tail(lines_with_vote)]
# Drop first four
lines_with_vote = lines_with_vote[5:length(lines_with_vote)] # Careful, only run this line once
# Find position where 'vote' starts
vote_vector = c()
for (x in lines_with_vote){
number_position = gregexpr(' vote',stack_html[x])[[1]][1]  # Find where in the line the word 'vote' appears
number_string = substring(stack_html[x], number_position-1, number_position) # Find the number before it
votes = as.numeric(number_string)   # Convert strings to numbers
vote_vector = c(vote_vector,votes)  # Attach number to vector
}
hist(vote_vector)
install.packages(c("logging", "RSQLite"))
library(logging)
basicConfig()
addHandler(writeToFile, logger="logger_name", file="logFileTest.txt")
loginf0("Test", logger="logger_name")
loginfo("Test", logger="logger_name")
list.files()
install.packages("RODBC")
html_code = function(url){
html = tryCatch(
{
suppressWarnings(readLines(url))
},
error=function(error_condition){ # executes only if there is an error in the block above
message('There was an error reading your url.') # generates a diagnostic looking print statement
message(paste('Here is the error:',error_condition))
},
warning=function(warning_condition){# Executes only if there was a warning in the block above
message('Warning!')
message(warning_condition)
},
finally={ # Executes always, no matter the outcome
message('\n All Done!')
}
)
return(html)
}
html_code('http://www.thiswebsitedoesntexist.com/123123123')
html_code('http://www.york.ac.uk/teaching/cws/wws/webpage1.html')
setwd("/Users/voitel/TRAINING/UW_Data_Science/UW_Data_Science_350/Week2")
#-----------------------------
#
#  Weather Scraper Function
#
#  Purpose: Get and store data from wunderground
#
#  Created by: Nikola Tesla (ntesla@uw.edu)
#
#  Created on: 2015-05-07
#
#-----------------------------
options(run.main=FALSE)
##----Import Libraries-----
require(RSQLite)
require(logging)
##----Define the get-weather-data function-----
get_weather_data = function(airport, dates, logger=NA, db_conn=NA){
# Build HTML Link String
site_prefix = 'http://www.wunderground.com/history/airport/'
site_suffix = '/DailyHistory.html?format=1'
weather_links = paste0(site_prefix,airport,'/',gsub('-','/',dates),site_suffix)
# Initialize final data frame
weather_frame = data.frame()
# Get Data
for (l in weather_links){
print(paste('Getting link',l))
# Log each attempt
if (is.function(logger)){
loginfo(paste('Getting link',l),logger=logger)
}
weather_info = tryCatch(readLines(l)[-1], # Get String Response
error = function(e){
print(paste('Error getting',l)) # Output Error on Screen/Console
if(is.function(logger)){loginfo(paste('Error getting',l)
,logger=logger)} # Store Error in Log
})
weather_info = strsplit(weather_info,',') # Parse each line by  a comma
headers = weather_info[[1]]               # Get Headers
weather_info = weather_info[-1]           # Drop Headers
# Now transform list into data frame
weather_info = do.call(rbind.data.frame, weather_info)
names(weather_info) = headers
# Post Retrieval Data Cleanup
weather_info <- data.frame(lapply(weather_info, as.character),
stringsAsFactors=FALSE)
# Convert numeric columns to numbers
numeric_cols = c(2,3,4,5,6,8,9,10,13)
weather_info[numeric_cols] = lapply(weather_info[numeric_cols],as.numeric)
# Fill in the 'dashes' to zero
weather_info[is.na(weather_info)]=0
# Rename the date column and drop the last html tag
colnames(weather_info)[14]="Date"
weather_info$Date = as.Date(substr(weather_info$Date,1,10))
# Concatenate DFs together
weather_frame = tryCatch(rbind(weather_frame, setNames(weather_info, names(weather_frame))),
error=function(e) {print(e);weather_frame})
} # End loop through each day's weather csv link (l)
# Log ending time
if(is.function(logger)){
loginfo('All done!',logger=logger)
}
# Write to SQLite DB
if(isS4(db_conn)){
dbWriteTable(db_conn, airport, weather_frame, overwrite=TRUE)
}
return(weather_frame)
}
if(interactive()){
##----Setup Test Logger-----
basicConfig()
addHandler(writeToFile, file="~/testing.log", level='DEBUG')
##----Test Parameters----
airport = 'KSEA'
dates = seq(from=as.Date('2015-05-01'),
to=as.Date('2015-05-06'),
by=1)
sql_db_name = 'weather.db'
##----Connect to SQLite DB----
con = dbConnect(SQLite(), dbname=sql_db_name)
weather_data = get_weather_data(airport, dates, logger=writeToFile, db_conn=con)
dbDisconnect(con)
}
install.packages("dplyr")
library(dplyr)
library(data.table)
source('weather_retrieval.R')
library(dplyr)
library(data.table)
source('weather_retrieval.R')
library(dplyr)
library(data.table)
source('weather_retrieval.R')
library(dplyr)
library(data.table)
source('weather_retrieval.R')
headcount = read.csv('JitteredHeadCount.csv', stringsAsFactors = FALSE)
headcount$DateFormat = as.Date(headcount$DateFormat, format="%m/%d/%Y")
weather_file_name = 'las_vegas_hourly_weather.csv'
headcount = read.csv('JitteredHeadCount.csv', stringsAsFactors = FALSE)
list.files()
headcount = read.csv('JitteredHeadCount.csv', stringsAsFactors = FALSE)
library(dplyr)
library(data.table)
source('weather_retrieval.R')
# Load jittered Data
headcount = read.csv('JitteredHeadCount.csv', stringsAsFactors = FALSE)
headcount$DateFormat = as.Date(headcount$DateFormat, format="%m/%d/%Y")
weather_file_name = 'las_vegas_hourly_weather.csv'
setwd("/Users/voitel/TRAINING/UW_Data_Science/UW_Data_Science_350/Week2")
headcount = read.csv('JitteredHeadCount.csv', stringsAsFactors = FALSE)
headcount$DateFormat = as.Date(headcount$DateFormat, format="%m/%d/%Y")
headcount = read.csv('JitteredHeadCount.csv', stringsAsFactors = FALSE)
headcount$DateFormat = as.Date(headcount$DateFormat, format="%m/%d/%Y")
weather_file_name = 'las_vegas_hourly_weather.csv'
range(headcount$DateFormat)
airport = 'KLAS'
dates = seq(from=min(headcount$DateFormat),
to=max(headcount$DateFormat),
by=1)
weather_data = get_weather_data(airport, dates)
names(weather_data) = c('time','temp','dew_pt','humidity','pressure',
'visibility','wind_dir','wind_speed','gust_speed',
'precipitation','events','conditions',
'wind_dir_deg','date')
# Let's create a datetime in the weather data
weather_data$datetime = paste(weather_data$date,weather_data$time)
weather_data$datetime = strptime(weather_data$datetime, format="%Y-%m-%d %I:%M %p")
weather_data$Hour = as.numeric(format(round(weather_data$datetime, units="hours"), format="%H"))
anyDuplicated(headcount[c("DateFormat", "Hour","GameCode")])
anyDuplicated(weather_data[c("date", 'Hour')]) # Oh no!  How could this happen?
weather_data = weather_data[!duplicated(weather_data[c("date", 'Hour')]),]
intersect(names(headcount), names(weather_data))
weather_data$DateFormat = weather_data$date
intersect(names(headcount), names(weather_data))
weather_data$date = NULL
headcount_base_all = merge(headcount, weather_data, all.x=TRUE, by=c("DateFormat","Hour"))
weather_data$datetime = NULL
library(data.table)
headcount = as.data.table(headcount)
weather_data = as.data.table(weather_data)
setkeyv(headcount, c("DateFormat", "Hour"))
setkeyv(weather_data, c("DateFormat", "Hour"))

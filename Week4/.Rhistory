install.packages("plyr")
install.packages("ggplot2")
g = rnorm(10,5)
plot(g)
pwd()
getwd()
install.packages("UsingR")
ls -l
exit()
install.packages(c("boot", "class", "cluster", "codetools", "colorspace", "evaluate", "foreign", "formatR", "Formula", "ggplot2", "highr", "Hmisc", "jsonlite", "KernSmooth", "knitr", "lattice", "manipulate", "markdown", "MASS", "Matrix", "mgcv", "mime", "nlme", "nnet", "plyr", "Rcpp", "RCurl", "rpart", "spatial", "stringr", "survival"))
library(data.table)
library(logging)
library(RSQLite)
install.packages("data.table", "logging", "RSQLite")
install.packages("data.table")
install.packages('logging')
install.packages(RSQLite)
install.packages("RSQLite")
library(data.table)
library(logging)
library(RSQLite)
list.dirs
A_matrix = matrix(4, nrow=4, ncol=3) # Makes use of broadcasting
View(A_matrix)
B_matrix = matrix(1:50, nrow=4, ncol=3)
View(B_matrix)
A_matrix + B_matrix
A_matrix * B_matrix
A_matrix %*% B_matrix
A_matrix %*% t(B_matrix)
x_values = seq(from=as.Date('2015-01-01'),
to=as.Date('2015-02-12'),
by = 3)
df = data.frame('dates' = x_values,
'x1'    = runif(15,-10,20),
'x2'    = 1:15,
'x3'    = strsplit('MississippiMath','')[[1]])
View(df)
df$x3 = as.character(df$x3)
df$x3 = tolower(df$x3)
View(df)
str(df)
head(df)
tail(df, n=10)
df = as.data.table(df)
View(df)
df[,sum(x1)]
df[,c(sum(x1),sd(x2))]
?logging
x_values
install.packages("XML")
nfl_site = "http://www.usatoday.com/sports/nfl/arrests/"
nfl_html = readHTMLTable(nfl_site)
nfl_site = "http://www.usatoday.com/sports/nfl/arrests/"
library(XML)
nfl_html = readHTMLTable(nfl_site)
nfl_html = readHTMLTable(nfl_site)
nfl_html
nfl_data = nfl_html[[1]]
nfl_html
db_file = 'test_db.db'
conn = dbConnect(dbDriver("SQLite"), dbname=db_file)
dbWriteTable(conn, 'table_name', medals_data, overwrite=TRUE)
medals_data <- read.table("medals.txt", sep="\t", header=TRUE)
p = 0.75
n = 1000
?rbinom
bern_samples = rbinom(n, 1, p)
bern_sample_mean = sum(bern_samples)/length(bern_samples)
bern_sample_var = bern_sample_mean * (1-bern_sample_mean)
##--------------------------------------------
##
## Exploring Data in R (lecture 1)
##
## Class: PCE Data Science Methods Class
##
## Contains examples of:
##
## -Working with Distributions
##
## -Visually/Numerically Exploring data
##
##--------------------------------------------
##-----Exploring data Visually----
# Bernoulli (Binomial with n = 1)
p = 0.75
n = 1000
bern_samples = rbinom(n, 1, p)
bern_sample_mean = sum(bern_samples)/length(bern_samples)
bern_sample_var = bern_sample_mean * (1-bern_sample_mean)
bern_var = p*(1-p)
# Binomial
N = c(5, 25, 75)
binom_samples = lapply(N, function(x) rbinom(n, x, p))
binom_sample_means = lapply(binom_samples, mean)
binom_means = N*p
binom_sample_vars = lapply(binom_samples, var)
binom_vars = N*p*(1-p)
par(mfrow=c(2,2))
for (i in 1:3){
hist(binom_samples[[i]], main=paste(N[i],'Experiments'), freq=FALSE)
x_norm = seq(0,N[i], by = 0.025)
y_norm = dnorm(x_norm, mean=binom_means[i], sd=sqrt(binom_vars[i]))
lines(x_norm, y_norm)
}
setwd("/Users/voitel/TRAINING/UW_Data_Science/UW_Data_Science_350/Week4")
?grep
string_vec = c('i like beer', 'i don\'t like bears', 'double match beerbeer', 'not a match')
# grep - returns the index of matches
string_pattern = 'be'
grep(string_pattern, string_vec, perl=TRUE)
#grepl - returns a vector of logicals indicating a match
grepl(string_pattern, string_vec, perl=TRUE)
#sub - substitute expression for first occurence
sub('i', 'you', string_vec[1], perl=TRUE)
sub('i', 'you', string_vec[1], perl=TRUE)
# gsub - global substitute
gsub('i', 'you', string_vec[1], perl=TRUE)
# regexpr - returns a list of information about the first match
regexpr(string_pattern, string_vec, perl=TRUE)
library(stringi)
library(devtools)
library(httr)
library(twitteR)
library(stringr)
##-----Oath Setup-----
twit_cred = read.csv('twitter_cred.csv', stringsAsFactors=FALSE)
TWITTER_CONSUMER_KEY = twit_cred$TWITTER_CONSUMER_KEY
TWITTER_CONSUMER_SECRET = twit_cred$TWITTER_CONSUMER_SECRET
TWITTER_ACCESS_TOKEN = twit_cred$TWITTER_ACCESS_TOKEN
TWITTER_ACCESS_SECRET = twit_cred$TWITTER_ACCESS_SECRET
setup_twitter_oauth(TWITTER_CONSUMER_KEY, TWITTER_CONSUMER_SECRET,
TWITTER_ACCESS_TOKEN, TWITTER_ACCESS_SECRET)
wh_tweets <- userTimeline('barackobama', n=100)
pattern = '[Aa]merican?'
wh_tweets = sapply(wh_tweets, function(x) x$text)
grep(pattern, wh_tweets, perl=TRUE)
data = data.frame('index' = 1:100,
'var1' = rnorm(100),
'group' = sample(1:3, 100, replace=TRUE, prob=c(0.1,0.45,0.45)))
data = data.frame('index' = 1:100,
'var1' = rnorm(100),
'group' = sample(1:3, 100, replace=TRUE, prob=c(0.1,0.45,0.45)))
##----Bernoulli Sampling-----
p = 0.1
bernoulli_sample = data[runif(100)<p,] # Yay for R vectorization
##----Cluster Sampling-----
num_clusters = 10
cluster_samples = lapply(1:num_clusters, function(x){
stopifnot((nrow(data) %% num_clusters)==0)
cluster_labels = sample(rep(1:num_clusters,each=num_clusters))
return(data[cluster_labels == x,])
})
size = 15
simple_random_sample = data[sample(1:nrow(data), size),]
split_data = split(data, list(data$group))
strat_samples_list <- lapply(split_data, function(x) x[sample(1:nrow(x), 5, FALSE),])
strat_sample <- do.call(rbind, strat_samples_list)
k = 5 # 100/5 = 20 observations
sys_sample_even = data[seq(1,nrow(data), by = k),]
k = 6 # 100/6 = 16.67 obs? hrm... do this:
# Pick a random start point between 1 and k:
start = sample(1:k,1)
sys_sample = data[seq(start,nrow(data), by=k),]
install.packages("ggplot2")
dbinom(x=10, size=60, prob=p_six)
# roll a dice 600 times, find p(x=100)
dbinom(x=100, size=600, prob=p_six)
p_six = 1/6
dbinom(x=10, size=60, prob=p_six)
dbinom(x=100, size=600, prob=p_six)
pbinom(12, size=60, prob=p_six) - pbinom(7, size=60, prob=p_six)
sum(sapply(8:12, function(x) dbinom(x, size=60, prob=p_six)))
# 2) p(70<x<130|600 trails)
pbinom(129, size=600, prob=p_six) - pbinom(70, size=600, prob=p_six)
# alternatively
sum(sapply(71:129, function(x) dbinom(x, size=600, prob=p_six)))
# View Distributions:
x_60 = 1:60
y_60 = dbinom(x_60, size=60, prob=p_six)
x_600 = 1:150
y_600 = dbinom(x_600, size=600, prob=p_six)
plot(x_60, y_60, type='l', main='Roll a Die 60 or 600 Times', xlab="# of Successes",
ylab="Probability", lwd=2, col="green", xlim=c(1,150))
lines(x_600, y_600, lwd=2, col="blue")
legend("topright", c("Roll 60 Times", "Roll 600 Times"), col=c("green", "blue"),
lty=c(1,1), lwd=c(2,2))
##----Coin Flips-----
# Calculate a running average of N-trials of flipping a fair coin
n = 10000
outcomes = round(runif(n))
running_average = sapply(1:n, function(x) mean(outcomes[1:x]))
plot(running_average, type='l')
grid()
outcomes_sd = sd(outcomes)
outcomes_sd_theo = sqrt( 0.5 * (1 - 0.5) )
outcomes_sd
outcomes_sd_theo
prob_normal = function(a, b, mean=0, sd=1){
stopifnot(a<=b) # Similar to an assert
return(pnorm(b,mean,sd) - pnorm(a,mean,sd))
}
prob_normal(20.1262055, Inf, 15, 4)
prob_normal(20.1262055, Inf, 15, 4)
prob_normal(-Inf,Inf)
prob_normal(-1,1)
prob_normal(-2,2)
prob_normal(-3,3)
cutoff_stat = function(alpha, mean=0, sd=1, one_tailed=TRUE){
stopifnot((alpha>0) & (alpha<1))
if (one_tailed){
return(qnorm(1-alpha, mean, sd))
}else{
return(qnorm(1-(alpha/2), mean, sd))
}
}
cutoff_stat(0.1, 15, 4)
n = seq(10,10000,len=1000)
sample_means = sapply(n, function(x) mean(rnorm(x)))
sample_sds = sapply(n, function(x) sd(rnorm(x)))
plot(n, sample_means) # Plot means
plot(n, sample_means) # Plot means
lines(n, 1/sqrt(n))   # Plot means +- st. error
lines(n, -1/sqrt(n))
plot(n, sample_sds)   # Plot sd's
lines(n, 1/sqrt(n)+1) # plot sd's +- st. error
lines(n, -1/sqrt(n)+1)
plot(n, sample_means) # Plot means
lines(n, 1/sqrt(n))   # Plot means +- st. error
lines(n, -1/sqrt(n))
plot(n, sample_sds)   # Plot sd's
lines(n, 1/sqrt(n)+1) # plot sd's +- st. error
lines(n, -1/sqrt(n)+1)
sample1 = rnorm(100, mean=2, sd=0.5)
sample2 = rnorm(100, mean=2, sd=0.05)
t.test(sample2, sample1)
sample1 = rnorm(100, mean=2, sd=0.5)
sample2 = rnorm(100, mean=2, sd=0.05)
t.test(sample1, sample2)
sample1 = rnorm(100, mean=2, sd=0.5)
sample2 = rnorm(100, mean=2, sd=0.05)
t.test(sample1, sample2)
sample1 = rnorm(100, mean=2, sd=0.5)
sample2 = rnorm(100, mean=2, sd=0.05)
t.test(sample1, sample2)
sample1 = rnorm(100, mean=2, sd=0.5)
sample2 = rnorm(100, mean=2, sd=0.05)
t.test(sample1, sample2)
sample1 = rnorm(100, mean=2, sd=0.5)
sample2 = rnorm(100, mean=2, sd=0.05)
t.test(sample1, sample2)
sample1 = rnorm(100, mean=2, sd=0.5)
sample2 = rnorm(100, mean=2, sd=0.05)
t.test(sample1, sample2)
sample1 = rnorm(100, mean=2, sd=0.5)
sample2 = rnorm(100, mean=2, sd=0.05)
t.test(sample1, sample2)
sample1 = rnorm(100, mean=2, sd=0.5)
sample2 = rnorm(100, mean=2, sd=0.05)
t.test(sample1, sample2)
sample1 = rnorm(100, mean=2, sd=0.5)
sample2 = rnorm(100, mean=2, sd=0.05)
t.test(sample1, sample2)
sample1 = rnorm(100, mean=2, sd=0.5)
sample2 = rnorm(100, mean=2, sd=0.05)
t.test(sample1, sample2)
sample1 = rnorm(100, mean=2, sd=0.5)
sample2 = rnorm(100, mean=2, sd=0.05)
t.test(sample1, sample2)
sample1 = rnorm(100, mean=2, sd=0.5)
sample2 = rnorm(100, mean=2, sd=0.05)
t.test(sample1, sample2)
sample1 = rnorm(100, mean=4, sd=0.5)
sample2 = rnorm(100, mean=2, sd=0.05)
t.test(sample1, sample2)
ab_data = data.frame('occurrence'=c(55,43,22),
'expected_per'=c(0.6,0.3,0.1))
chisq.test(ab_data$occurrence, p = ab_data$expected_per)
ab_data
chisq.test(ab_data$occurrence, p = ab_data$expected_per)
1-pchisq(13.708, df=2) # 13.708 from slides
